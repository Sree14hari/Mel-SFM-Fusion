{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de67ba20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Running on cuda\n",
      "üéØ Classes: {'Cysts_Structural': 0, 'Dysarthia': 1, 'Hyperfunctional': 2, 'Laryngitis': 3, 'Vox senilis': 4, 'parkinson': 5, 'spasmodische_dysphonie': 6}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# ================= 1. CONFIG =================\n",
    "CONFIG = {\n",
    "    \"feature_dir\": Path(\"features\"),\n",
    "    \"batch_size\": 8,\n",
    "    \"lr\": 5e-5,\n",
    "    \"epochs\": 20,\n",
    "    \"d_model\": 64,\n",
    "    \"nhead\": 4,\n",
    "    \"num_layers\": 2,\n",
    "    \"ssl_dim\": 768,\n",
    "    \"sfm_dim\": 7,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"checkpoint_path\": \"best_model.pth\"\n",
    "}\n",
    "\n",
    "print(f\"‚öôÔ∏è Running on {CONFIG['device']}\")\n",
    "\n",
    "# ================= 2. GLOBAL CLASS MAP =================\n",
    "TRAIN_SSL_DIR = CONFIG[\"feature_dir\"] / \"ssl\" / \"train\"\n",
    "CLASSES = sorted([d.name for d in TRAIN_SSL_DIR.iterdir() if d.is_dir()])\n",
    "CLASS_TO_IDX = {cls: i for i, cls in enumerate(CLASSES)}\n",
    "\n",
    "print(f\"üéØ Classes: {CLASS_TO_IDX}\")\n",
    "\n",
    "# ================= 3. DATASET =================\n",
    "class DualStreamDataset(Dataset):\n",
    "    def __init__(self, split=\"train\"):\n",
    "        self.ssl_dir = CONFIG[\"feature_dir\"] / \"ssl\" / split\n",
    "        self.sfm_dir = CONFIG[\"feature_dir\"] / \"sfm\" / split\n",
    "        self.files = []\n",
    "\n",
    "        for cls in CLASSES:\n",
    "            ssl_cls = self.ssl_dir / cls\n",
    "            sfm_cls = self.sfm_dir / cls\n",
    "\n",
    "            for f in ssl_cls.glob(\"*.npy\"):\n",
    "                sfm_path = sfm_cls / f.name\n",
    "                if sfm_path.exists():\n",
    "                    self.files.append({\n",
    "                        \"ssl\": f,\n",
    "                        \"sfm\": sfm_path,\n",
    "                        \"label\": CLASS_TO_IDX[cls]\n",
    "                    })\n",
    "\n",
    "        print(f\"‚úÖ Loaded {len(self.files)} samples for '{split}'\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.files[idx]\n",
    "        ssl = torch.from_numpy(np.load(item[\"ssl\"])).float()\n",
    "        sfm = torch.from_numpy(np.load(item[\"sfm\"])).float()\n",
    "        label = torch.tensor(item[\"label\"]).long()\n",
    "        return ssl, sfm, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    ssl, sfm, labels = zip(*batch)\n",
    "\n",
    "    ssl_pad = pad_sequence(ssl, batch_first=True, padding_value=0.0)\n",
    "    sfm_pad = pad_sequence(sfm, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    lengths = torch.tensor([x.size(0) for x in ssl])\n",
    "    mask = torch.arange(ssl_pad.size(1))[None, :] < lengths[:, None]\n",
    "\n",
    "    return ssl_pad, sfm_pad, mask.bool(), torch.stack(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40dde6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= 4. MODEL (CORRECTED FOR TORCH VERSION) =================\n",
    "class DualStreamTransformer(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        d = CONFIG[\"d_model\"]\n",
    "\n",
    "        # Projections\n",
    "        self.ssl_proj = nn.Linear(CONFIG[\"ssl_dim\"], d)\n",
    "        self.sfm_proj = nn.Linear(CONFIG[\"sfm_dim\"], d)\n",
    "\n",
    "        self.ssl_norm = nn.LayerNorm(d)\n",
    "        self.sfm_norm = nn.LayerNorm(d)\n",
    "\n",
    "        # Independent encoders (NO weight sharing)\n",
    "        self.ssl_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d,\n",
    "                nhead=CONFIG[\"nhead\"],\n",
    "                dim_feedforward=256,\n",
    "                dropout=0.1,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=CONFIG[\"num_layers\"]\n",
    "        )\n",
    "\n",
    "        self.sfm_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d,\n",
    "                nhead=CONFIG[\"nhead\"],\n",
    "                dim_feedforward=256,\n",
    "                dropout=0.1,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=CONFIG[\"num_layers\"]\n",
    "        )\n",
    "\n",
    "        # Cross-attention (NO average_attn_weights here)\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=d,\n",
    "            num_heads=CONFIG[\"nhead\"],\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fusion_norm = nn.LayerNorm(d)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, ssl, sfm, mask):\n",
    "        \"\"\"\n",
    "        ssl : (B, T, 768)\n",
    "        sfm : (B, T, 7)\n",
    "        mask: (B, T)  True = valid, False = padding\n",
    "        \"\"\"\n",
    "\n",
    "        # Project + normalize\n",
    "        ssl = self.ssl_norm(self.ssl_proj(ssl))\n",
    "        sfm = self.sfm_norm(self.sfm_proj(sfm))\n",
    "\n",
    "        # Encode streams\n",
    "        ssl = self.ssl_encoder(ssl, src_key_padding_mask=~mask)\n",
    "        sfm = self.sfm_encoder(sfm, src_key_padding_mask=~mask)\n",
    "\n",
    "        # Cross-attention\n",
    "        fused, attn_weights = self.cross_attn(\n",
    "            query=ssl,\n",
    "            key=sfm,\n",
    "            value=sfm,\n",
    "            key_padding_mask=~mask,\n",
    "            need_weights=True,\n",
    "            average_attn_weights=False   # ‚úÖ correct place\n",
    "        )\n",
    "        # attn_weights shape: (B, nhead, T, T)\n",
    "\n",
    "        # Residual + norm\n",
    "        fused = self.fusion_norm(fused + ssl)\n",
    "\n",
    "        # Masked pooling\n",
    "        mask_exp = mask.unsqueeze(-1).float()\n",
    "        pooled = (fused * mask_exp).sum(dim=1) / (mask_exp.sum(dim=1) + 1e-8)\n",
    "\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits, attn_weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef068dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= 5. TRAINING (PURE TERMINAL OUTPUT) =================\n",
    "from tqdm.std import tqdm\n",
    "\n",
    "def train():\n",
    "    train_ds = DualStreamDataset(\"train\")\n",
    "    val_ds   = DualStreamDataset(\"val\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    model = DualStreamTransformer(len(CLASSES)).to(CONFIG[\"device\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"lr\"], weight_decay=1e-4)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    print(\"\\nüöÄ Training Started\")\n",
    "    print(\"Epoch | Train Loss | Val Loss | Val Acc\")\n",
    "    print(\"-\" * 45)\n",
    "\n",
    "    for epoch in range(CONFIG[\"epochs\"]):\n",
    "\n",
    "        # ---------- TRAIN ----------\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for ssl, sfm, mask, labels in tqdm(\n",
    "            train_loader,\n",
    "            desc=f\"Epoch {epoch+1}/{CONFIG['epochs']}\",\n",
    "            ncols=80,\n",
    "            ascii=True\n",
    "        ):\n",
    "            ssl, sfm, mask, labels = (\n",
    "                ssl.to(CONFIG[\"device\"]),\n",
    "                sfm.to(CONFIG[\"device\"]),\n",
    "                mask.to(CONFIG[\"device\"]),\n",
    "                labels.to(CONFIG[\"device\"])\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(ssl, sfm, mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # ---------- VALIDATION ----------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for ssl, sfm, mask, labels in val_loader:\n",
    "                ssl, sfm, mask, labels = (\n",
    "                    ssl.to(CONFIG[\"device\"]),\n",
    "                    sfm.to(CONFIG[\"device\"]),\n",
    "                    mask.to(CONFIG[\"device\"]),\n",
    "                    labels.to(CONFIG[\"device\"])\n",
    "                )\n",
    "\n",
    "                outputs, _ = model(ssl, sfm, mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"{epoch+1:>5} | {train_loss:.4f} | {val_loss:.4f} | {val_acc:.2f}%\")\n",
    "\n",
    "        # ---------- CHECKPOINT ----------\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), CONFIG[\"checkpoint_path\"])\n",
    "            print(\"   üíæ Best model saved\")\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c2499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 14740 samples for 'train'\n",
      "‚úÖ Loaded 920 samples for 'val'\n",
      "\n",
      "üöÄ Training Started\n",
      "Epoch | Train Loss | Val Loss | Val Acc\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|###########################| 1843/1843 [01:30<00:00, 20.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1 | 1.0033 | 0.5160 | 83.91%\n",
      "   üíæ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|###########################| 1843/1843 [00:40<00:00, 45.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2 | 0.4972 | 0.4466 | 85.98%\n",
      "   üíæ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|###########################| 1843/1843 [00:39<00:00, 46.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3 | 0.4351 | 0.4349 | 85.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|###########################| 1843/1843 [00:38<00:00, 47.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4 | 0.3997 | 0.4242 | 86.52%\n",
      "   üíæ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|###########################| 1843/1843 [00:41<00:00, 44.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    5 | 0.3835 | 0.4442 | 86.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|###########################| 1843/1843 [00:42<00:00, 43.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    6 | 0.3625 | 0.4549 | 87.07%\n",
      "   üíæ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|###########################| 1843/1843 [01:17<00:00, 23.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    7 | 0.3449 | 0.4253 | 88.04%\n",
      "   üíæ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|###########################| 1843/1843 [01:06<00:00, 27.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8 | 0.3318 | 0.4935 | 85.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|###########################| 1843/1843 [01:08<00:00, 27.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9 | 0.3179 | 0.4209 | 87.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|##########################| 1843/1843 [00:59<00:00, 30.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10 | 0.3061 | 0.4418 | 87.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|##########################| 1843/1843 [00:59<00:00, 30.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11 | 0.2936 | 0.4086 | 88.80%\n",
      "   üíæ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|##########################| 1843/1843 [01:01<00:00, 29.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12 | 0.2833 | 0.4790 | 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|##########################| 1843/1843 [01:00<00:00, 30.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   13 | 0.2724 | 0.4732 | 88.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|##########################| 1843/1843 [00:47<00:00, 38.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14 | 0.2601 | 0.4616 | 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|##########################| 1843/1843 [00:46<00:00, 40.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   15 | 0.2510 | 0.4434 | 88.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|##########################| 1843/1843 [01:08<00:00, 26.99it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================= 6. RUN =================\n",
    "model, history = train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf55b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================= 7. PLOT =================\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"train_loss\"], label=\"Train\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Val\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(); plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history[\"val_acc\"], label=\"Val Acc\", color=\"green\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.legend(); plt.grid()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
