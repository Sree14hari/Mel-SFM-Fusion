{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T04:45:23.666285Z",
     "start_time": "2026-01-02T04:45:18.963840Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm  # specific for Jupyter progress bars\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# Based on your screenshot\n",
    "SOURCE_DIR = Path(\"processed_data\")\n",
    "OUTPUT_DIR = Path(\"dataset_split\")\n",
    "\n",
    "# Extensions to look for (add .mp3 or others if needed)\n",
    "EXTENSIONS = (\"*.wav\", \"*.flac\")\n",
    "\n",
    "# Split Ratios\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO   = 0.1\n",
    "TEST_RATIO  = 0.1\n",
    "\n",
    "SEED = 42\n",
    "# =================================================\n",
    "\n",
    "def split_dataset_notebook():\n",
    "    # 1. Setup\n",
    "    random.seed(SEED)\n",
    "\n",
    "    if not SOURCE_DIR.exists():\n",
    "        print(f\"‚ùå Error: Folder '{SOURCE_DIR}' not found. Check your path!\")\n",
    "        return\n",
    "\n",
    "    # Detect classes automatically from subfolders\n",
    "    classes = [d.name for d in SOURCE_DIR.iterdir() if d.is_dir()]\n",
    "    classes.sort()\n",
    "\n",
    "    print(f\"üìÇ Found {len(classes)} classes: {classes}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # 2. Iterate and Split\n",
    "    # We use tqdm to show a progress bar in Jupyter\n",
    "    for cls in tqdm(classes, desc=\"Processing Classes\"):\n",
    "\n",
    "        # Gather files\n",
    "        cls_dir = SOURCE_DIR / cls\n",
    "        files = []\n",
    "        for ext in EXTENSIONS:\n",
    "            files.extend(list(cls_dir.glob(ext)))\n",
    "\n",
    "        # Sort to ensure reproducibility before shuffling\n",
    "        files.sort()\n",
    "\n",
    "        if not files:\n",
    "            print(f\"‚ö†Ô∏è Warning: No audio files found in '{cls}'\")\n",
    "            continue\n",
    "\n",
    "        # --- Stratified Split Logic ---\n",
    "        # 1. Split Train vs (Val + Test)\n",
    "        train_files, temp_files = train_test_split(\n",
    "            files, test_size=(1 - TRAIN_RATIO), random_state=SEED, shuffle=True\n",
    "        )\n",
    "\n",
    "        # 2. Split Val vs Test (adjusting ratio for the smaller temp set)\n",
    "        val_relative_ratio = VAL_RATIO / (VAL_RATIO + TEST_RATIO)\n",
    "        val_files, test_files = train_test_split(\n",
    "            temp_files, test_size=(1 - val_relative_ratio), random_state=SEED, shuffle=True\n",
    "        )\n",
    "\n",
    "        # --- Copying Files ---\n",
    "        splits = {\n",
    "            \"train\": train_files,\n",
    "            \"val\":   val_files,\n",
    "            \"test\":  test_files\n",
    "        }\n",
    "\n",
    "        for split_name, split_files in splits.items():\n",
    "            save_dir = OUTPUT_DIR / split_name / cls\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            for f in split_files:\n",
    "                shutil.copy2(f, save_dir / f.name)\n",
    "\n",
    "    # 3. Final Summary\n",
    "    print(\"\\n‚úÖ Dataset Split Complete!\")\n",
    "    print(f\"   Output Location: {OUTPUT_DIR.resolve()}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"{'Class':<25} | {'Train':<5} | {'Val':<5} | {'Test':<5}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    for cls in classes:\n",
    "        n_train = len(list((OUTPUT_DIR / \"train\" / cls).glob(\"*\")))\n",
    "        n_val   = len(list((OUTPUT_DIR / \"val\" / cls).glob(\"*\")))\n",
    "        n_test  = len(list((OUTPUT_DIR / \"test\" / cls).glob(\"*\")))\n",
    "        print(f\"{cls:<25} | {n_train:<5} | {n_val:<5} | {n_test:<5}\")\n",
    "\n",
    "# Run the function\n",
    "split_dataset_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356bd38de5f7402",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T04:49:56.417677Z",
     "start_time": "2026-01-02T04:49:21.295317Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, Gain, Shift\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "TRAIN_DIR = Path(\"dataset_split/train\")\n",
    "EXTENSIONS = (\"*.wav\", \"*.flac\")\n",
    "\n",
    "# Define Safe Augmentations for Voice Pathology\n",
    "augmenter = Compose([\n",
    "    # Add varying levels of noise (simulates different recording environments)\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "\n",
    "    # Change speed slightly without changing pitch (preserves formants)\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "\n",
    "    # Change volume (robustness to microphone gain)\n",
    "    # FIX 1: 'min_gain_in_db' -> 'min_gain_db'\n",
    "    Gain(min_gain_db=-12, max_gain_db=12, p=0.5),\n",
    "\n",
    "    # Shift time (start signal slightly later/earlier)\n",
    "    # FIX 2: 'min_fraction' -> 'min_shift' (defaults to fraction unit)\n",
    "    Shift(min_shift=-0.1, max_shift=0.1, p=0.5),\n",
    "])\n",
    "# =================================================\n",
    "\n",
    "def run_offline_augmentation():\n",
    "    if not TRAIN_DIR.exists():\n",
    "        print(f\"‚ùå Error: {TRAIN_DIR} does not exist. Run the split step first.\")\n",
    "        return\n",
    "\n",
    "    # Get all classes in train\n",
    "    classes = [d.name for d in TRAIN_DIR.iterdir() if d.is_dir()]\n",
    "    total_new_files = 0\n",
    "\n",
    "    print(f\"üöÄ Starting Augmentation on {len(classes)} classes...\")\n",
    "\n",
    "    for cls in classes:\n",
    "        cls_dir = TRAIN_DIR / cls\n",
    "\n",
    "        # Gather original files\n",
    "        files = []\n",
    "        for ext in EXTENSIONS:\n",
    "            files.extend(list(cls_dir.glob(ext)))\n",
    "\n",
    "        # Filter out files that are ALREADY augmented (if you run this twice)\n",
    "        files = [f for f in files if \"_aug\" not in f.name]\n",
    "\n",
    "        print(f\"   ‚Ä¢ {cls}: Augmenting {len(files)} files...\")\n",
    "\n",
    "        for file_path in tqdm(files, desc=f\"Augmenting {cls}\", leave=False):\n",
    "            try:\n",
    "                # 1. Load Audio\n",
    "                # We use librosa to load as float32, which audiomentations expects\n",
    "                y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "                # 2. Apply Augmentation\n",
    "                augmented_y = augmenter(samples=y, sample_rate=sr)\n",
    "\n",
    "                # 3. Save\n",
    "                # Create output filename: \"original_aug.wav\"\n",
    "                out_name = file_path.stem + \"_aug\" + file_path.suffix\n",
    "                out_path = cls_dir / out_name\n",
    "\n",
    "                sf.write(out_path, augmented_y, sr)\n",
    "                total_new_files += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ö†Ô∏è Error processing {file_path.name}: {e}\")\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"‚úÖ Augmentation Complete!\")\n",
    "    print(f\"üéâ Created {total_new_files} new training samples.\")\n",
    "    print(f\"üìÇ Check folder: {TRAIN_DIR}\")\n",
    "\n",
    "# Run it\n",
    "run_offline_augmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1577fa19cd84a31d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-02T04:51:15.256734Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading wav2vec2 on cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941561b0941c4fb4b3fae8054676dfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Found 16585 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16585/16585 [57:44<00:00,  4.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ EXTRACTION COMPLETE\n",
      "Saved to: A:\\Conferences\\VOICE\\features\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "\n",
    "# ================= CONFIG =================\n",
    "DATASET_DIR = Path(\"dataset_split\")\n",
    "OUTPUT_DIR  = Path(\"features\")\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-base-960h\"\n",
    "TARGET_SR = 16000\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "BATCH_SIZE = 32   # reduce to 4 if VRAM < 8GB\n",
    "# =========================================\n",
    "\n",
    "\n",
    "def align_features(feat: np.ndarray, target_len: int):\n",
    "    \"\"\"Linear interpolation for temporal alignment\"\"\"\n",
    "    x_old = np.linspace(0, 1, feat.shape[0])\n",
    "    x_new = np.linspace(0, 1, target_len)\n",
    "    return np.stack([\n",
    "        np.interp(x_new, x_old, feat[:, i])\n",
    "        for i in range(feat.shape[1])\n",
    "    ], axis=1)\n",
    "\n",
    "\n",
    "def extract_features():\n",
    "\n",
    "    print(f\"üîÑ Loading wav2vec2 on {DEVICE}\")\n",
    "    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "    model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Create output directories\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for cls in (DATASET_DIR / split).iterdir():\n",
    "            if cls.is_dir():\n",
    "                (OUTPUT_DIR / \"ssl\" / split / cls.name).mkdir(parents=True, exist_ok=True)\n",
    "                (OUTPUT_DIR / \"sfm\" / split / cls.name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    audio_files = list(DATASET_DIR.rglob(\"*.wav\")) + list(DATASET_DIR.rglob(\"*.flac\"))\n",
    "    print(f\"üöÄ Found {len(audio_files)} files\")\n",
    "\n",
    "    batch_audio = []\n",
    "    batch_paths = []\n",
    "\n",
    "    for wav_path in tqdm(audio_files, desc=\"Extracting features\"):\n",
    "\n",
    "        try:\n",
    "            y, _ = librosa.load(wav_path, sr=TARGET_SR)\n",
    "            batch_audio.append(y)\n",
    "            batch_paths.append(wav_path)\n",
    "\n",
    "            # ---------- PROCESS BATCH ----------\n",
    "            if len(batch_audio) == BATCH_SIZE:\n",
    "                process_batch(batch_audio, batch_paths, processor, model)\n",
    "                batch_audio, batch_paths = [], []\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed loading {wav_path.name}: {e}\")\n",
    "\n",
    "    # Process remaining files\n",
    "    if len(batch_audio) > 0:\n",
    "        process_batch(batch_audio, batch_paths, processor, model)\n",
    "\n",
    "    print(\"\\n‚úÖ EXTRACTION COMPLETE\")\n",
    "    print(f\"Saved to: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "\n",
    "def process_batch(batch_audio, batch_paths, processor, model):\n",
    "\n",
    "    # ================= SSL (GPU) =================\n",
    "    inputs = processor(\n",
    "        batch_audio,\n",
    "        sampling_rate=TARGET_SR,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            inputs.input_values.to(DEVICE),\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "\n",
    "    # Average last 4 layers\n",
    "    ssl_batch = torch.stack(outputs.hidden_states[-4:]).mean(0)\n",
    "    ssl_batch = ssl_batch.cpu().numpy()  # (B, T, 768)\n",
    "\n",
    "    # ================= PER FILE: SFM + SAVE =================\n",
    "    for i, wav_path in enumerate(batch_paths):\n",
    "\n",
    "        ssl_feat = ssl_batch[i]\n",
    "        T = ssl_feat.shape[0]\n",
    "\n",
    "        y, _ = librosa.load(wav_path, sr=TARGET_SR)\n",
    "\n",
    "        # -------- SOURCE FEATURES --------\n",
    "        f0, _, voiced_prob = librosa.pyin(\n",
    "            y, fmin=50, fmax=500, sr=TARGET_SR\n",
    "        )\n",
    "        f0 = np.nan_to_num(f0)\n",
    "        voiced_prob = np.nan_to_num(voiced_prob)\n",
    "\n",
    "        energy = librosa.feature.rms(y=y)[0]\n",
    "\n",
    "        # -------- FILTER FEATURES (LPC) --------\n",
    "        lpc = librosa.lpc(y, order=12)\n",
    "        lpc = np.abs(lpc[:4])\n",
    "\n",
    "        min_len = min(len(f0), len(energy))\n",
    "        f0 = f0[:min_len]\n",
    "        energy = energy[:min_len]\n",
    "        voiced_prob = voiced_prob[:min_len]\n",
    "\n",
    "        lpc_feat = np.repeat(lpc[:, None], min_len, axis=1)\n",
    "\n",
    "        sfm_raw = np.vstack([\n",
    "            f0,\n",
    "            energy,\n",
    "            voiced_prob,\n",
    "            lpc_feat\n",
    "        ]).T\n",
    "\n",
    "        sfm_feat = align_features(sfm_raw, T)\n",
    "\n",
    "        # -------- SAVE --------\n",
    "        rel = wav_path.relative_to(DATASET_DIR)\n",
    "        ssl_path = OUTPUT_DIR / \"ssl\" / rel.with_suffix(\".npy\")\n",
    "        sfm_path = OUTPUT_DIR / \"sfm\" / rel.with_suffix(\".npy\")\n",
    "\n",
    "        np.save(ssl_path, ssl_feat.astype(np.float32))\n",
    "        np.save(sfm_path, sfm_feat.astype(np.float32))\n",
    "\n",
    "\n",
    "# ================= RUN =================\n",
    "extract_features()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
